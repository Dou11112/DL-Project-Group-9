{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"UW6RduE8OKTo"},"outputs":[],"source":["import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.linear_model import LinearRegression, Ridge, Lasso\n","from sklearn.preprocessing import LabelEncoder, StandardScaler\n","from sklearn.metrics import mean_squared_error, r2_score\n","import numpy as np"]},{"cell_type":"markdown","source":["# We start off by training debugging dataset, just to see if the model is working, as in it produces at least some meaningful values"],"metadata":{"id":"8pfY_lCZWapm"}},{"cell_type":"code","source":["# open the dataset pickles\n","\n","debugging_dataset = pd.read_pickle('debugging_dataset.pkl')\n","working_dataset = pd.read_pickle('working_dataset.pkl')"],"metadata":{"id":"4OVYqjcyOovU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Treat differently for categorical and numerical features. also make sure to transform the values properly"],"metadata":{"id":"l1JBvlQLWfcD"}},{"cell_type":"code","source":["grade_columns = ['A+', 'A', 'A-', 'B+', 'B', 'B-', 'C+', 'C', 'C-', 'D+', 'D', 'D-', 'F', 'W']\n","debugging_dataset[grade_columns] = debugging_dataset[grade_columns].apply(pd.to_numeric, errors='coerce').fillna(0)\n","\n","# Define categorical columns and encode them consistently across the dataset\n","categorical_columns = ['Year', 'Term', 'Subject', 'Sched Type', 'Number', 'Course Title']\n","for column in categorical_columns:\n","    le = LabelEncoder()\n","    debugging_dataset[column] = le.fit_transform(debugging_dataset[column].astype(str))"],"metadata":{"id":"TaZpgbItPVO6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_data, temp_data = train_test_split(debugging_dataset, test_size=0.3, random_state=42)\n","val_data, test_data = train_test_split(temp_data, test_size=0.5, random_state=42)\n","\n","print(\"Train set shape:\", train_data.shape)\n","print(\"Validation set shape:\", val_data.shape)\n","print(\"Test set shape:\", test_data.shape)"],"metadata":{"id":"G7xVd0sIPmjB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1733935198852,"user_tz":360,"elapsed":302,"user":{"displayName":"Dou Hoon Kwark","userId":"05134277220026188692"}},"outputId":"7b86cb99-b7ed-4e29-ac81-df6f71c69ec6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Train set shape: (2065, 20)\n","Validation set shape: (443, 20)\n","Test set shape: (443, 20)\n"]}]},{"cell_type":"code","source":["feature_columns = categorical_columns\n","target_columns = grade_columns"],"metadata":{"id":"doNYwkCQRUAn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_train = train_data[feature_columns]\n","y_train = train_data[target_columns]\n","X_val = val_data[feature_columns]\n","y_val = val_data[target_columns]\n","X_test = test_data[feature_columns]\n","y_test = test_data[target_columns]\n","\n","scaler = StandardScaler()\n","X_train = scaler.fit_transform(X_train)\n","X_val = scaler.transform(X_val)\n","X_test = scaler.transform(X_test)\n"],"metadata":{"id":"lbwCzeFcR0xZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## We train simple basic Linear regression model for each grade (A+, A, ...)"],"metadata":{"id":"_J1Yl8KgYk8H"}},{"cell_type":"code","source":["models = {}\n","for grade in target_columns:\n","    model = LinearRegression()\n","    model.fit(X_train, y_train[grade])\n","    models[grade] = model"],"metadata":{"id":"8ZyrpVGuR4TQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["predictions = {}\n","for grade, model in models.items():\n","    predictions[grade] = model.predict(X_test)"],"metadata":{"id":"N0seY_JER6NQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Comparing the model prediction against the groundtruth grade distribution"],"metadata":{"id":"nXEWeEb8Yv0N"}},{"cell_type":"code","source":["pred_df = pd.DataFrame(predictions, columns=target_columns)\n","actual_grade_distribution = y_test.mean(axis=0) * 100\n","\n","predicted_grade_distribution = pred_df.mean(axis=0) * 100\n","\n","comparison_df = pd.DataFrame({\n","    'Actual': actual_grade_distribution,\n","    'Predicted': predicted_grade_distribution\n","})\n","\n","print(\"\\nComparison of Actual and Predicted Grade Distribution (Percentage):\")\n","print(comparison_df)\n","\n","for grade in target_columns:\n","    y_test_actual = y_test[grade]\n","    y_pred = pred_df[grade]\n","    mse = mean_squared_error(y_test_actual, y_pred)\n","    r2 = r2_score(y_test_actual, y_pred)\n","    print(f\"\\nGrade: {grade}\")\n","    print(f\"Mean Squared Error (MSE): {mse}\")\n","    print(f\"R-squared (R2): {r2}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dvbOcd1WSYdw","executionInfo":{"status":"ok","timestamp":1733935265885,"user_tz":360,"elapsed":270,"user":{"displayName":"Dou Hoon Kwark","userId":"05134277220026188692"}},"outputId":"e5beafe1-a456-4692-dd9a-d9cdb12c7d01"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Comparison of Actual and Predicted Grade Distribution (Percentage):\n","       Actual  Predicted\n","A+  11.644910  11.655937\n","A   32.437908  32.846352\n","A-  14.352393  14.127400\n","B+  10.782024  10.560539\n","B   12.253517  12.202918\n","B-   5.260435   5.188847\n","C+   3.176939   3.169433\n","C    3.831894   3.889185\n","C-   1.649604   1.656959\n","D+   0.777099   0.788391\n","D    1.142750   1.161973\n","D-   0.485665   0.496514\n","F    1.823044   1.887710\n","W    0.381817   0.367840\n","\n","Grade: A+\n","Mean Squared Error (MSE): 0.026920251266652383\n","R-squared (R2): 0.03147160381927472\n","\n","Grade: A\n","Mean Squared Error (MSE): 0.048375403801239975\n","R-squared (R2): 0.018721808232958104\n","\n","Grade: A-\n","Mean Squared Error (MSE): 0.011783569947907985\n","R-squared (R2): 0.01078671115769081\n","\n","Grade: B+\n","Mean Squared Error (MSE): 0.007679107339471414\n","R-squared (R2): 0.015203758020584268\n","\n","Grade: B\n","Mean Squared Error (MSE): 0.009385058167175431\n","R-squared (R2): 0.008156800316001078\n","\n","Grade: B-\n","Mean Squared Error (MSE): 0.0030987017343739568\n","R-squared (R2): 0.008668541091983228\n","\n","Grade: C+\n","Mean Squared Error (MSE): 0.0017633513810639898\n","R-squared (R2): 0.006109541456820189\n","\n","Grade: C\n","Mean Squared Error (MSE): 0.0025965604881630563\n","R-squared (R2): 0.0075823525455129825\n","\n","Grade: C-\n","Mean Squared Error (MSE): 0.000844053269873502\n","R-squared (R2): 0.008028414440027443\n","\n","Grade: D+\n","Mean Squared Error (MSE): 0.00032624771816151175\n","R-squared (R2): 0.007880936285579176\n","\n","Grade: D\n","Mean Squared Error (MSE): 0.0004964649123412186\n","R-squared (R2): 0.004409014905634279\n","\n","Grade: D-\n","Mean Squared Error (MSE): 0.00019693031503824403\n","R-squared (R2): 0.005088749709137641\n","\n","Grade: F\n","Mean Squared Error (MSE): 0.001081868833345656\n","R-squared (R2): 0.025175578126030995\n","\n","Grade: W\n","Mean Squared Error (MSE): 0.00014008978367088787\n","R-squared (R2): 0.0013361747246296574\n"]}]},{"cell_type":"markdown","source":["## The model prediction on average\n","\n","Altough the model prediction is not very accruate, we now have a good evidence that the model works. Therefore, we now move on to the model training with dataset of more data points"],"metadata":{"id":"Vkuw0IUHbFbA"}},{"cell_type":"code","source":["mae = np.mean(np.abs(actual_grade_distribution - predicted_grade_distribution))\n","print(f\"Mean Absolute Error (MAE) of Predicted Grade Distribution: {mae:.4f}%\")\n","\n","rmse = np.sqrt(np.mean((actual_grade_distribution - predicted_grade_distribution) ** 2))\n","print(f\"Root Mean Squared Error (RMSE) of Predicted Grade Distribution: {rmse:.4f}%\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UqMrXxz7SbVy","executionInfo":{"status":"ok","timestamp":1733935275476,"user_tz":360,"elapsed":139,"user":{"displayName":"Dou Hoon Kwark","userId":"05134277220026188692"}},"outputId":"acc3cd89-3a21-4cb0-bb74-b195da3023cd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mean Absolute Error (MAE) of Predicted Grade Distribution: 0.0843%\n","Root Mean Squared Error (RMSE) of Predicted Grade Distribution: 0.1421%\n"]}]},{"cell_type":"markdown","source":["# We will train 2 linear regression models with regularizer--Ridge, and Lasso Regression models"],"metadata":{"id":"4ircrhm_d6xa"}},{"cell_type":"code","source":["param_grids = {\n","    'Ridge Regression': {\n","        'alpha': [0.1, 1.0, 10.0, 100.0]\n","    },\n","    'Lasso Regression': {\n","        'alpha': [0.001, 0.01, 0.1, 1.0]\n","    }\n","}"],"metadata":{"id":"GDjvWFdNeZyx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Again, we split the data and convert them into proper values for model training"],"metadata":{"id":"ZS37xZ7vbc1d"}},{"cell_type":"code","source":["grade_columns = ['A+', 'A', 'A-', 'B+', 'B', 'B-', 'C+', 'C', 'C-', 'D+', 'D', 'D-', 'F', 'W']\n","working_dataset[grade_columns] = working_dataset[grade_columns].apply(pd.to_numeric, errors='coerce').fillna(0)\n","\n","categorical_columns = ['Year', 'Term', 'Subject', 'Sched Type', 'Number', 'Course Title']\n","for column in categorical_columns:\n","    le = LabelEncoder()\n","    working_dataset[column] = le.fit_transform(working_dataset[column].astype(str))"],"metadata":{"id":"1YTm6TxWV7UA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_data, temp_data = train_test_split(working_dataset, test_size=0.3, random_state=42)\n","val_data, test_data = train_test_split(temp_data, test_size=0.5, random_state=42)\n","\n","print(\"Train set shape:\", train_data.shape)\n","print(\"Validation set shape:\", val_data.shape)\n","print(\"Test set shape:\", test_data.shape)\n","\n","feature_columns = categorical_columns\n","target_columns = grade_columns"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"M2w5L5Y7WAIa","executionInfo":{"status":"ok","timestamp":1733935279644,"user_tz":360,"elapsed":21,"user":{"displayName":"Dou Hoon Kwark","userId":"05134277220026188692"}},"outputId":"62424c08-a4d2-441d-d22f-8046452aa51e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Train set shape: (41311, 20)\n","Validation set shape: (8852, 20)\n","Test set shape: (8853, 20)\n"]}]},{"cell_type":"code","source":["X_train = train_data[feature_columns]\n","y_train = train_data[target_columns]\n","X_val = val_data[feature_columns]\n","y_val = val_data[target_columns]\n","X_test = test_data[feature_columns]\n","y_test = test_data[target_columns]\n","\n","# Scale the features\n","scaler = StandardScaler()\n","X_train = scaler.fit_transform(X_train)\n","X_val = scaler.transform(X_val)\n","X_test = scaler.transform(X_test)"],"metadata":{"id":"UNqVU8IlWO5d"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Now, we will train the models with hyper-parameter (namely regularizer value) tuning on the validation set"],"metadata":{"id":"QJcG5G12fHPM"}},{"cell_type":"code","source":["val_results = {}\n","\n","# Iterate over each model and its parameter grid\n","for name, param_grid in param_grids.items():\n","    print(f\"\\nTuning {name}...\")\n","\n","    if name == 'Ridge Regression':\n","        base_model = Ridge()\n","    elif name == 'Lasso Regression':\n","        base_model = Lasso()\n","\n","    grid_search = GridSearchCV(base_model, param_grid, cv=3, scoring='neg_mean_absolute_error', n_jobs=-1)\n","\n","    mae_scores = []\n","    rmse_scores = []\n","    for grade in target_columns:\n","        grid_search.fit(X_train, y_train[grade])\n","        best_model = grid_search.best_estimator_\n","        y_val_pred = best_model.predict(X_val)\n","\n","        mae = np.mean(np.abs(y_val[grade] - y_val_pred))\n","        rmse = np.sqrt(np.mean((y_val[grade] - y_val_pred) ** 2))\n","\n","        mae_scores.append(mae)\n","        rmse_scores.append(rmse)\n","\n","    avg_mae = np.mean(mae_scores)\n","    avg_rmse = np.mean(rmse_scores)\n","    val_results[name] = {\n","        'Best Params': grid_search.best_params_,\n","        'MAE': avg_mae,\n","        'RMSE': avg_rmse\n","    }\n","\n","    print(f\"{name} - Best Params: {grid_search.best_params_}, Validation MAE: {avg_mae:.4f}, Validation RMSE: {avg_rmse:.4f}\")\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"o62QKiIdWPKI","executionInfo":{"status":"ok","timestamp":1733935300967,"user_tz":360,"elapsed":10805,"user":{"displayName":"Dou Hoon Kwark","userId":"05134277220026188692"}},"outputId":"3cae43fd-0132-46a3-fe27-ffc352b736c6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Tuning Ridge Regression...\n","Ridge Regression - Best Params: {'alpha': 0.1}, Validation MAE: 0.0516, Validation RMSE: 0.0681\n","\n","Tuning Lasso Regression...\n","Lasso Regression - Best Params: {'alpha': 0.001}, Validation MAE: 0.0516, Validation RMSE: 0.0681\n"]}]},{"cell_type":"markdown","source":["## it turns out the selected best hyper parmaters are small, and each best model prediction result is very similar"],"metadata":{"id":"zWrx00OQhb-_"}},{"cell_type":"code","source":["print(\"\\nHyperparameter Tuning Results:\")\n","for model_name, result in val_results.items():\n","    print(f\"{model_name}: Best Params = {result['Best Params']}, MAE = {result['MAE']:.4f}, RMSE = {result['RMSE']:.4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XfmOPT5-fFOy","executionInfo":{"status":"ok","timestamp":1733935300968,"user_tz":360,"elapsed":23,"user":{"displayName":"Dou Hoon Kwark","userId":"05134277220026188692"}},"outputId":"d512eec8-2064-4662-9408-0da181a3143b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Hyperparameter Tuning Results:\n","Ridge Regression: Best Params = {'alpha': 0.1}, MAE = 0.0516, RMSE = 0.0681\n","Lasso Regression: Best Params = {'alpha': 0.001}, MAE = 0.0516, RMSE = 0.0681\n"]}]},{"cell_type":"markdown","source":["## We further compute the numbers on the test set"],"metadata":{"id":"HJdWNoIJhp6z"}},{"cell_type":"code","source":["best_model_name = min(val_results, key=lambda x: val_results[x]['MAE'])\n","\n","print(f'best model chosen is {best_model_name}')\n","best_model_params = val_results[best_model_name]['Best Params']\n","\n","if best_model_name == 'Ridge Regression':\n","    final_model = Ridge(**best_model_params)\n","elif best_model_name == 'Lasso Regression':\n","    final_model = Lasso(**best_model_params)\n","\n","X_train_val = np.vstack((X_train, X_val))\n","y_train_val = pd.concat([y_train, y_val])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"psz5-u5vgl1d","executionInfo":{"status":"ok","timestamp":1733935300968,"user_tz":360,"elapsed":19,"user":{"displayName":"Dou Hoon Kwark","userId":"05134277220026188692"}},"outputId":"4d466d73-5d72-4f36-f1f1-061255559331"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["best model chosen is Ridge Regression\n"]}]},{"cell_type":"code","source":["# Initialize an empty dictionary for predictions\n","predictions = {}\n","\n","for grade in target_columns:\n","    final_model.fit(X_train_val, y_train_val[grade])\n","    predictions[grade] = final_model.predict(X_test)\n","\n","pred_df = pd.DataFrame(predictions, columns=target_columns)\n","\n","actual_grade_distribution = y_test.values\n","predicted_grade_distribution = pred_df.values\n","\n","distribution_mae = np.mean(np.abs(actual_grade_distribution - predicted_grade_distribution))\n","distribution_rmse = np.sqrt(np.mean((actual_grade_distribution - predicted_grade_distribution) ** 2))\n","\n","actual_grade_distribution = actual_grade_distribution * 100\n","predicted_grade_distribution = predicted_grade_distribution * 100\n","\n","comparison_df = pd.DataFrame({\n","    'Grade': target_columns,\n","    'Actual Mean': actual_grade_distribution.mean(axis=0),\n","    'Predicted Mean': predicted_grade_distribution.mean(axis=0),\n","})\n","\n","print(\"\\nComparison of Actual and Predicted Grade Distribution:\")\n","print(comparison_df)\n","\n","print(f\"\\nOverall MAE of Grade Distribution: {distribution_mae:.4f} ({distribution_mae*100:.4f}%)\")\n","print(f\"Overall RMSE of Grade Distribution: {distribution_rmse:.4f} ({distribution_rmse*100:.4f}%)\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e2tdvYlznB7j","executionInfo":{"status":"ok","timestamp":1733938020206,"user_tz":360,"elapsed":447,"user":{"displayName":"Dou Hoon Kwark","userId":"05134277220026188692"}},"outputId":"2af2d480-0abd-4708-a49c-edd3e2be0592"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Comparison of Actual and Predicted Grade Distribution:\n","   Grade  Actual Mean  Predicted Mean\n","0     A+    11.644910       11.696920\n","1      A    32.437908       32.883158\n","2     A-    14.352393       14.128598\n","3     B+    10.782024       10.527335\n","4      B    12.253517       12.198097\n","5     B-     5.260435        5.171094\n","6     C+     3.176939        3.153001\n","7      C     3.831894        3.876271\n","8     C-     1.649604        1.654795\n","9     D+     0.777099        0.790311\n","10     D     1.142750        1.165648\n","11    D-     0.485665        0.501753\n","12     F     1.823044        1.886891\n","13     W     0.381817        0.366128\n","\n","Overall MAE of Grade Distribution: 0.0518 (5.1803%)\n","Overall RMSE of Grade Distribution: 0.0911 (9.1062%)\n"]}]},{"cell_type":"markdown","source":["## It shows that with the current linear regression model (Ridge Regression), our prediction is 5.1803% off and 9.1062% off from the groundtruth in MAE and RMSE, respectively. These numbers will serve as baseline numbers for upcoming milestones"],"metadata":{"id":"UbjUfEfPhv49"}},{"cell_type":"code","source":[],"metadata":{"id":"p-d3XlBgWX2U"},"execution_count":null,"outputs":[]}]}
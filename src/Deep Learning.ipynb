{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Similar to Baseline Learning, we start off by training model on debugging dataset in the same manner described in the Baseline Learning notebook"],"metadata":{"id":"30DmKKsGpx5x"}},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","from sklearn.preprocessing import OneHotEncoder\n","from sklearn.model_selection import train_test_split\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","\n"],"metadata":{"id":"KJfP_GM6phuV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Setting up the datasets, both main and debugging"],"metadata":{"id":"U6xfHtjBr9bS"}},{"cell_type":"code","source":["debugging_dataset = pd.read_pickle('debugging_dataset.pkl')\n","working_dataset = pd.read_pickle('working_dataset.pkl')\n","\n","# Grade columns (GT)\n","grade_columns = ['A+', 'A', 'A-', 'B+', 'B', 'B-', 'C+', 'C', 'C-', 'D+', 'D', 'D-', 'F', 'W']\n","\n","# Categorical columns (features)\n","categorical_columns = ['Year', 'Term', 'Subject', 'Sched Type', 'Number', 'Course Title']\n"],"metadata":{"id":"WmoeYdYLpwS2","executionInfo":{"status":"error","timestamp":1733908103908,"user_tz":360,"elapsed":26,"user":{"displayName":"Xi Teng","userId":"06254976700694448969"}},"outputId":"f619f767-4628-4d82-d1e2-10554c246912","colab":{"base_uri":"https://localhost:8080/","height":365}},"execution_count":null,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: 'debugging_dataset.pkl'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-95c30796c900>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdebugging_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'debugging_dataset.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mworking_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'working_dataset.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Grade columns (GT)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mgrade_columns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'A+'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'A'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'A-'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'B+'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'B'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'B-'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'C+'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'C'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'C-'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'D+'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'D'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'D-'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'F'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'W'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/pickle.py\u001b[0m in \u001b[0;36mread_pickle\u001b[0;34m(filepath_or_buffer, compression, storage_options)\u001b[0m\n\u001b[1;32m    183\u001b[0m     \"\"\"\n\u001b[1;32m    184\u001b[0m     \u001b[0mexcs_to_catch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mAttributeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModuleNotFoundError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m     with get_handle(\n\u001b[0m\u001b[1;32m    186\u001b[0m         \u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m         \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    880\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    881\u001b[0m             \u001b[0;31m# Binary mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 882\u001b[0;31m             \u001b[0mhandle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    883\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'debugging_dataset.pkl'"]}]},{"cell_type":"markdown","source":["# Dataloader Definition and initialization"],"metadata":{"id":"J6n8_jr1tJ1U"}},{"cell_type":"markdown","source":["## We will one-hot encode each category first, computing required embedding space for each category."],"metadata":{"id":"BMwBXcwpn1QY"}},{"cell_type":"code","source":["max_categories = {\n","    col: max(debugging_dataset[col].nunique(), working_dataset[col].nunique())\n","    for col in categorical_columns\n","}\n","print(\"Maximum categories for each column:\", max_categories)\n"],"metadata":{"id":"k9xHSaelxni8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["split manner is as follow: 0.75: 0.15: 0.15 for train, val, test"],"metadata":{"id":"KAL2XwA-oD5a"}},{"cell_type":"code","source":["train_ratio = 0.7\n","val_ratio = 0.15\n","test_ratio = 0.15\n","\n","debug_train, debug_temp = train_test_split(debugging_dataset, test_size=(1 - train_ratio), random_state=42)\n","debug_val, debug_test = train_test_split(debug_temp, test_size=(test_ratio / (val_ratio + test_ratio)), random_state=42)\n","\n","work_train, work_temp = train_test_split(working_dataset, test_size=(1 - train_ratio), random_state=42)\n","work_val, work_test = train_test_split(work_temp, test_size=(test_ratio / (val_ratio + test_ratio)), random_state=42)\n","\n","print(f\"Debugging Train: {len(debug_train)}, Val: {len(debug_val)}, Test: {len(debug_test)}\")\n","print(f\"Working Train: {len(work_train)}, Val: {len(work_val)}, Test: {len(work_test)}\")\n"],"metadata":{"id":"j1ed5PZjc0Ws"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def one_hot_encode_fixed(dataframe, column, max_categories):\n","    encoder = OneHotEncoder(sparse_output=False, handle_unknown=\"ignore\")\n","    one_hot = encoder.fit_transform(dataframe[[column]])\n","\n","    padded = np.zeros((len(dataframe), max_categories))\n","    padded[:, :one_hot.shape[1]] = one_hot\n","    return padded\n","\n","def encode_categorical_features(dataframe, categorical_columns, max_categories):\n","    one_hot_encoded = {\n","        col: one_hot_encode_fixed(dataframe, col, max_categories[col])\n","        for col in categorical_columns\n","    }\n","    return one_hot_encoded\n"],"metadata":{"id":"28IfSo91c1Gl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["debug_train_encoded = encode_categorical_features(debug_train, categorical_columns, max_categories)\n","debug_val_encoded = encode_categorical_features(debug_val, categorical_columns, max_categories)\n","debug_test_encoded = encode_categorical_features(debug_test, categorical_columns, max_categories)\n","\n","work_train_encoded = encode_categorical_features(work_train, categorical_columns, max_categories)\n","work_val_encoded = encode_categorical_features(work_val, categorical_columns, max_categories)\n","work_test_encoded = encode_categorical_features(work_test, categorical_columns, max_categories)\n","\n","for col in categorical_columns:\n","    print(f\"{col} (Debug Train): {debug_train_encoded[col].shape}\")\n","    print(f\"{col} (Work Train): {work_train_encoded[col].shape}\")\n"],"metadata":{"id":"LB21mxzMc6gW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Build the dataloader and initliaize it, making sure to shuffle the training sets to avoid overfitting"],"metadata":{"id":"VOGFWdE9pMRr"}},{"cell_type":"code","source":["class GradeDataset(Dataset):\n","    def __init__(self, encoded_features, grade_columns, targets_dataframe):\n","        self.encoded_features = {col: torch.tensor(encoded_features[col], dtype=torch.float32) for col in encoded_features}\n","        self.targets = torch.tensor(targets_dataframe[grade_columns].values, dtype=torch.float32)\n","\n","    def __len__(self):\n","        return len(self.targets)\n","\n","    def __getitem__(self, idx):\n","        # we further drop the feature 'Course Title'\n","        features = {col: self.encoded_features[col][idx] for col in self.encoded_features if col != 'Course Title'}\n","        # features = {col: self.encoded_features[col][idx] for col in self.encoded_features}\n","        return features, self.targets[idx]\n"],"metadata":{"id":"lxlsGkRHc-0A"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["debug_train_dataset = GradeDataset(debug_train_encoded, grade_columns, debug_train)\n","debug_val_dataset = GradeDataset(debug_val_encoded, grade_columns, debug_val)\n","debug_test_dataset = GradeDataset(debug_test_encoded, grade_columns, debug_test)\n","\n","work_train_dataset = GradeDataset(work_train_encoded, grade_columns, work_train)\n","work_val_dataset = GradeDataset(work_val_encoded, grade_columns, work_val)\n","work_test_dataset = GradeDataset(work_test_encoded, grade_columns, work_test)\n","\n","debug_train_loader = DataLoader(debug_train_dataset, batch_size=64, shuffle=True)\n","debug_val_loader = DataLoader(debug_val_dataset, batch_size=64, shuffle=False)\n","debug_test_loader = DataLoader(debug_test_dataset, batch_size=64, shuffle=False)\n","\n","work_train_loader = DataLoader(work_train_dataset, batch_size=64, shuffle=True)\n","work_val_loader = DataLoader(work_val_dataset, batch_size=64, shuffle=False)\n","work_test_loader = DataLoader(work_test_dataset, batch_size=64, shuffle=False)\n","\n"],"metadata":{"id":"DN5TGwAedHpS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# check the features in the datset\n","for features, targets in debug_train_loader:\n","    print(f\"Features keys and shapes: {features.keys()}\")\n","    print(f\"Targets shape: {targets.shape}\")\n","    break\n","for features, targets in work_train_loader:\n","    print(f\"Features keys and shapes: {features.keys()}\")\n","    print(f\"Targets shape: {targets.shape}\")\n","    break"],"metadata":{"id":"vCq3rOoK3GlX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Build 2-layer NN model"],"metadata":{"id":"fpCqbAW3oj1w"}},{"cell_type":"code","source":["import torch.nn.functional as F\n","import torch.nn as nn\n","class GradePredictor(nn.Module):\n","    def __init__(self, feature_sizes, output_dim):\n","        super(GradePredictor, self).__init__()\n","\n","        # Create layers for each feature\n","        self.feature_layers = nn.ModuleDict({\n","            col: nn.Sequential(\n","                nn.Linear(size, 64),\n","                nn.ReLU(),\n","                nn.Dropout(0.3)\n","            )\n","            for col, size in feature_sizes.items()\n","        })\n","\n","        # Combine outputs from all features\n","        self.fc = nn.Sequential(\n","            nn.Linear(len(feature_sizes) * 64, 128),\n","            nn.ReLU(),\n","            nn.Dropout(0.3),\n","            nn.Linear(128, output_dim)\n","        )\n","\n","    def forward(self, features):\n","        feature_outputs = [layer(features[col]) for col, layer in self.feature_layers.items()]\n","\n","        combined = torch.cat(feature_outputs, dim=1)\n","        logits = self.fc(combined)\n","\n","        percentages = F.softmax(logits, dim=1)\n","        return percentages\n"],"metadata":{"id":"q876NBOEdn9K"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### just small experiment--train model for each grade separately"],"metadata":{"id":"ZcHTdBXRPeMN"}},{"cell_type":"code","source":["class GradePredictor(nn.Module):\n","    def __init__(self, feature_sizes):\n","        super(GradePredictor, self).__init__()\n","        self.feature_layers = nn.ModuleDict({\n","            col: nn.Sequential(\n","                nn.Linear(size, 64),\n","                nn.ReLU(),\n","                nn.Dropout(0.3)\n","            )\n","            for col, size in feature_sizes.items()\n","        })\n","        self.fc = nn.Sequential(\n","            nn.Linear(len(feature_sizes) * 64, 128),\n","            nn.ReLU(),\n","            nn.Dropout(0.3),\n","            nn.Linear(128, 1)  # Output 1 value for this specific grade\n","        )\n","\n","    def forward(self, features):\n","        feature_outputs = [layer(features[col]) for col, layer in self.feature_layers.items()]\n","        combined = torch.cat(feature_outputs, dim=1)\n","        output = self.fc(combined)\n","        return output\n","\n","\n","def train_model_for_grade(grade, train_loader, val_loader, feature_sizes, epochs=20, lr=0.0001):\n","    print(f\"\\nTraining model for grade: {grade}\")\n","\n","    model = GradePredictor(feature_sizes).to(device)\n","    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n","    criterion = nn.MSELoss()\n","\n","    train_losses = []\n","    val_losses = []\n","\n","    for epoch in range(epochs):\n","        model.train()\n","        train_loss = 0.0\n","        for features, targets in train_loader:\n","            features = {col: features[col].to(device) for col in features}\n","            targets = targets[:, grade_columns.index(grade)].to(device)\n","            optimizer.zero_grad()\n","            outputs = model(features).squeeze(1)\n","            loss = criterion(outputs, targets)\n","            loss.backward()\n","            optimizer.step()\n","            train_loss += loss.item()\n","\n","        train_losses.append(train_loss / len(train_loader))\n","\n","        # Validation\n","        model.eval()\n","        val_loss = 0.0\n","        with torch.no_grad():\n","            for features, targets in val_loader:\n","                features = {col: features[col].to(device) for col in features}\n","                targets = targets[:, grade_columns.index(grade)].to(device)\n","                outputs = model(features).squeeze(1)\n","                loss = criterion(outputs, targets)\n","                val_loss += loss.item()\n","\n","        val_losses.append(val_loss / len(val_loader))\n","\n","        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss/len(train_loader):.4f}, Val Loss: {val_loss/len(val_loader):.4f}\")\n","\n","    return model, train_losses, val_losses\n","\n","\n","trained_models = {}\n","grade_results = {}\n","\n","for grade in grade_columns:\n","    model, train_losses, val_losses = train_model_for_grade(\n","        grade, debug_train_loader, debug_val_loader, feature_sizes, epochs=20, lr=0.0001\n","    )\n","    trained_models[grade] = model\n","    grade_results[grade] = {\n","        \"train_losses\": train_losses,\n","        \"val_losses\": val_losses\n","    }\n","\n","# Plot training and validation loss for each grade\n","for grade in grade_columns:\n","    plt.figure(figsize=(8, 5))\n","    plt.plot(grade_results[grade][\"train_losses\"], label=\"Train Loss\")\n","    plt.plot(grade_results[grade][\"val_losses\"], label=\"Validation Loss\")\n","    plt.title(f\"Loss for Grade: {grade}\")\n","    plt.xlabel(\"Epoch\")\n","    plt.ylabel(\"Loss\")\n","    plt.legend()\n","    plt.show()\n","\n"],"metadata":{"id":"l5TqzysbOWtu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# get the average loss\n","overall_train_losses = []\n","overall_val_losses = []\n","for grade in grade_columns:\n","    overall_train_losses.append(grade_results[grade][\"train_losses\"][-1])\n","    overall_val_losses.append(grade_results[grade][\"val_losses\"][-1])\n","\n","print(f\"Overall Train Loss: {np.mean(overall_train_losses):.4f}\")\n","print(f\"Overall Validation Loss: {np.mean(overall_val_losses):.4f}\")"],"metadata":{"id":"nDknsuqhPOhD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Train the model on debugging set for a dry-run"],"metadata":{"id":"wONp3d7gpWou"}},{"cell_type":"code","source":["feature_sizes = {col: debug_train_encoded[col].shape[1] for col in debug_train_encoded}\n","# drop the 'Course Title'\n","feature_sizes.pop('Course Title', None)\n","print(feature_sizes)\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","output_dim = len(grade_columns)\n","model = GradePredictor(feature_sizes, output_dim).to(device)\n","\n","criterion = nn.MSELoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n","\n","def train_model(model, train_loader, val_loader, optimizer, criterion, epochs=10):\n","    train_losses = []\n","    val_losses = []\n","    for epoch in range(epochs):\n","        model.train()\n","        train_loss = 0.0\n","        for features, targets in train_loader:\n","            features = {col: features[col].to(device) for col in features}\n","            targets = targets.to(device)\n","            optimizer.zero_grad()\n","            outputs = model(features)\n","            # loss = criterion(F.softmax(outputs, dim=1), targets)\n","            loss = criterion(outputs, targets)\n","            loss.backward()\n","            optimizer.step()\n","            train_loss += loss.item()\n","\n","        train_losses.append(train_loss / len(train_loader))\n","\n","        # Validation\n","        model.eval()\n","        val_loss = 0.0\n","        with torch.no_grad():\n","            for features, targets in val_loader:\n","                features = {col: features[col].to(device) for col in features}\n","                targets = targets.to(device)\n","                outputs = model(features)\n","                loss = criterion(outputs, targets)\n","                val_loss += loss.item()\n","\n","        val_losses.append(val_loss / len(val_loader))\n","\n","        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss/len(train_loader):.4f}, Val Loss: {val_loss/len(val_loader):.4f}\")\n","\n","    return train_losses, val_losses\n","\n","# Train\n","train_losses, val_losses = train_model(model, debug_train_loader, debug_val_loader, optimizer, criterion, epochs=20)\n","\n","# plot\n","import matplotlib.pyplot as plt\n","\n","plt.plot(train_losses, label='Train Loss')\n","plt.plot(val_losses, label='Validation Loss')\n","plt.xlabel('Epoch')\n","plt.ylabel('Loss')\n","plt.legend()\n","plt.show()\n","\n"],"metadata":{"id":"dFPyk3DndzUH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Now, we build the model on the main dataset and do the hyper parameter settings"],"metadata":{"id":"sZDIH4Ljv5SO"}},{"cell_type":"markdown","source":["# Analyze the effect of batch size with Adam --[16, 32, 128, 256, 512]\n","also, we furthered dropped the \"Course Title\" as it kept leading the model to overfitting"],"metadata":{"id":"tFQaOVKjwFMj"}},{"cell_type":"code","source":["batch_sizes = [16, 32, 64, 128, 256, 512]\n","batch_results = {}\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","for batch_size in batch_sizes:\n","    print(f\"\\nTraining with batch size: {batch_size}\")\n","\n","    train_loader = DataLoader(work_train_dataset, batch_size=batch_size, shuffle=True)\n","    val_loader = DataLoader(work_val_dataset, batch_size=batch_size, shuffle=False)\n","\n","    model = GradePredictor(feature_sizes, output_dim).to(device)\n","    optimizer = torch.optim.Adam(model.parameters(), lr=0.00001, weight_decay=1e-4)  # Added weight decay\n","    criterion = nn.MSELoss()\n","    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=2, factor=0.5)\n","\n","    train_losses, val_losses = [], []\n","    best_val_loss = float('inf')\n","    patience = 3\n","    trigger_times = 0\n","\n","    for epoch in range(10):\n","        model.train()\n","        train_loss = 0.0\n","        for features, targets in train_loader:\n","            features = {col: features[col].to(device) for col in features}\n","            targets = targets.to(device)\n","            optimizer.zero_grad()\n","            outputs = model(features)\n","            loss = criterion(outputs, targets)\n","            loss.backward()\n","            optimizer.step()\n","            train_loss += loss.item()\n","        train_losses.append(train_loss / len(train_loader))\n","\n","        model.eval()\n","        val_loss = 0.0\n","        with torch.no_grad():\n","            for features, targets in val_loader:\n","                features = {col: features[col].to(device) for col in features}\n","                targets = targets.to(device)\n","                outputs = model(features)\n","                loss = criterion(outputs, targets)\n","                val_loss += loss.item()\n","        val_losses.append(val_loss / len(val_loader))\n","\n","        scheduler.step(val_losses[-1])\n","\n","        print(f\"Epoch {epoch+1}, Train Loss: {train_losses[-1]:.4f}, Val Loss: {val_losses[-1]:.4f}\")\n","\n","        if val_loss < best_val_loss:\n","            best_val_loss = val_loss\n","            trigger_times = 0\n","        else:\n","            trigger_times += 1\n","            if trigger_times >= patience:\n","                print(\"Early stopping!\")\n","                break\n","\n","    batch_results[batch_size] = {\n","        'train_losses': train_losses,\n","        'val_losses': val_losses\n","    }\n","\n","# Plot results\n","plt.figure(figsize=(10, 6))\n","for batch_size, results in batch_results.items():\n","    plt.plot(results['val_losses'], label=f'Batch size {batch_size}')\n","plt.title('Validation Loss vs. Epochs for Batch Size')\n","plt.xlabel('Epochs')\n","plt.ylabel('Validation Loss')\n","plt.legend()\n","plt.show()"],"metadata":{"id":"0ns7SK7kzYvD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## As the model gets easily overfit to the current data points, with batch size(larger points to deal with), the model takes more epochs to converge"],"metadata":{"id":"jSqfepMvLohG"}},{"cell_type":"markdown","source":["## Then we investigate effects of different optimizers -- [SGD, Adam, RMSprop] with the batch size 64"],"metadata":{"id":"2ikBZT01s9rn"}},{"cell_type":"code","source":["import itertools\n","\n","optimizers = {\n","    'Adam': torch.optim.Adam,\n","    'SGD': torch.optim.SGD,\n","    'RMSprop': torch.optim.RMSprop\n","}\n","learning_rates = [0.0001, 0.00001, 0.000001]\n","\n","opt_lr_results = {}\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","for opt_name, opt_class in optimizers.items():\n","    for lr in learning_rates:\n","        print(f\"\\nTraining with optimizer: {opt_name}, Learning rate: {lr}\")\n","\n","        train_loader = DataLoader(work_train_dataset, batch_size=64, shuffle=True)\n","        val_loader = DataLoader(work_val_dataset, batch_size=64, shuffle=False)\n","\n","        model = GradePredictor(feature_sizes, output_dim).to(device)\n","        optimizer = opt_class(model.parameters(), lr=lr, weight_decay=1e-4)\n","        criterion = nn.MSELoss()\n","        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=2, factor=0.5)\n","\n","        train_losses, val_losses = [], []\n","        best_val_loss = float('inf')\n","        patience = 3\n","        trigger_times = 0\n","\n","        # Training\n","        for epoch in range(10):\n","            model.train()\n","            train_loss = 0.0\n","            for features, targets in train_loader:\n","                features = {col: features[col].to(device) for col in features}\n","                targets = targets.to(device)\n","                optimizer.zero_grad()\n","                outputs = model(features)\n","                loss = criterion(outputs, targets)\n","                loss.backward()\n","                optimizer.step()\n","                train_loss += loss.item()\n","            train_losses.append(train_loss / len(train_loader))\n","\n","            model.eval()\n","            val_loss = 0.0\n","            with torch.no_grad():\n","                for features, targets in val_loader:\n","                    features = {col: features[col].to(device) for col in features}\n","                    targets = targets.to(device)\n","                    outputs = model(features)\n","                    loss = criterion(outputs, targets)\n","                    val_loss += loss.item()\n","            val_losses.append(val_loss / len(val_loader))\n","\n","            scheduler.step(val_losses[-1])\n","\n","            print(f\"Epoch {epoch+1}, Train Loss: {train_losses[-1]:.4f}, Val Loss: {val_losses[-1]:.4f}\")\n","\n","            if val_loss < best_val_loss:\n","                best_val_loss = val_loss\n","                trigger_times = 0\n","            else:\n","                trigger_times += 1\n","                if trigger_times >= patience:\n","                    print(\"Early stopping!\")\n","                    break\n","\n","        opt_lr_results[(opt_name, lr)] = {\n","            'train_losses': train_losses,\n","            'val_losses': val_losses\n","        }\n","\n","# Plot results\n","plt.figure(figsize=(12, 8))\n","for (opt_name, lr), results in opt_lr_results.items():\n","    label = f\"{opt_name}, LR={lr}\"\n","    plt.plot(results['val_losses'], label=label)\n","plt.title('Validation Loss vs. Epochs for Optimizer and Learning Rate Combinations')\n","plt.xlabel('Epochs')\n","plt.ylabel('Validation Loss')\n","plt.legend()\n","plt.show()\n","\n","\n"],"metadata":{"id":"IlopyYY1sHZ_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## With the current dataset, it seems that Adam is most effective optimizer to find the best parameters."],"metadata":{"id":"aHqlbOnAL3z3"}},{"cell_type":"markdown","source":["## Now, we are doing more extensive hyper-parameter searching with dropout rates, weight decays, patience number for learning schedule, and hidden size for the model"],"metadata":{"id":"L8xmcxj5LW9U"}},{"cell_type":"code","source":["import itertools\n","\n","# Define hyperparameter ranges\n","dropout_rates = [0.2, 0.3, 0.5]\n","weight_decays = [0, 1e-4, 1e-3]\n","patiences = [2, 3, 5]\n","hidden_sizes = [64, 128, 256]  # Number of neurons in the hidden layers\n","\n","# Create a dictionary to store results\n","hyperparam_results = {}\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Iterate over all combinations of hyperparameters\n","for dropout, weight_decay, patience, hidden_size in itertools.product(dropout_rates, weight_decays, patiences, hidden_sizes):\n","    print(f\"\\nTraining with Dropout={dropout}, Weight Decay={weight_decay}, Patience={patience}, Hidden Size={hidden_size}\")\n","\n","    train_loader = DataLoader(work_train_dataset, batch_size=64, shuffle=True)\n","    val_loader = DataLoader(work_val_dataset, batch_size=64, shuffle=False)\n","\n","    # Define a modified model with the current hyperparameters\n","    class ModifiedGradePredictor(nn.Module):\n","        def __init__(self, feature_sizes, output_dim, dropout_rate, hidden_size):\n","            super().__init__()\n","            self.feature_layers = nn.ModuleDict({\n","                col: nn.Sequential(\n","                    nn.Linear(size, hidden_size),\n","                    nn.ReLU(),\n","                    nn.Dropout(dropout_rate)\n","                )\n","                for col, size in feature_sizes.items()\n","            })\n","            self.fc = nn.Sequential(\n","                nn.Linear(len(feature_sizes) * hidden_size, hidden_size),\n","                nn.ReLU(),\n","                nn.Dropout(dropout_rate),\n","                nn.Linear(hidden_size, output_dim)\n","            )\n","\n","        def forward(self, features):\n","            feature_outputs = [layer(features[col]) for col, layer in self.feature_layers.items()]\n","            combined = torch.cat(feature_outputs, dim=1)\n","            return F.softmax(self.fc(combined), dim=1)\n","\n","    model = ModifiedGradePredictor(feature_sizes, output_dim, dropout, hidden_size).to(device)\n","    optimizer = torch.optim.Adam(model.parameters(), lr=0.00001, weight_decay=weight_decay)\n","    criterion = nn.MSELoss()\n","    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=patience, factor=0.5)\n","\n","    train_losses, val_losses = [], []\n","    best_val_loss = float('inf')\n","    trigger_times = 0\n","\n","    for epoch in range(10):\n","        model.train()\n","        train_loss = 0.0\n","        for features, targets in train_loader:\n","            features = {col: features[col].to(device) for col in features}\n","            targets = targets.to(device)\n","            optimizer.zero_grad()\n","            outputs = model(features)\n","            loss = criterion(outputs, targets)\n","            loss.backward()\n","            optimizer.step()\n","            train_loss += loss.item()\n","        train_losses.append(train_loss / len(train_loader))\n","\n","        model.eval()\n","        val_loss = 0.0\n","        with torch.no_grad():\n","            for features, targets in val_loader:\n","                features = {col: features[col].to(device) for col in features}\n","                targets = targets.to(device)\n","                outputs = model(features)\n","                loss = criterion(outputs, targets)\n","                val_loss += loss.item()\n","        val_losses.append(val_loss / len(val_loader))\n","\n","        scheduler.step(val_losses[-1])\n","\n","        print(f\"Epoch {epoch+1}, Train Loss: {train_losses[-1]:.4f}, Val Loss: {val_losses[-1]:.4f}\")\n","\n","        if val_loss < best_val_loss:\n","            best_val_loss = val_loss\n","            trigger_times = 0\n","        else:\n","            trigger_times += 1\n","            if trigger_times >= patience:\n","                print(\"Early stopping!\")\n","                break\n","\n","    # Save results for the current hyperparameter combination\n","    hyperparam_results[(dropout, weight_decay, patience, hidden_size)] = {\n","        'train_losses': train_losses,\n","        'val_losses': val_losses\n","    }\n","\n","# Plot results\n","plt.figure(figsize=(12, 8))\n","for (dropout, weight_decay, patience, hidden_size), results in hyperparam_results.items():\n","    label = f\"Dropout={dropout}, WD={weight_decay}, Pat={patience}, H={hidden_size}\"\n","    plt.plot(results['val_losses'], label=label)\n","plt.title('Validation Loss vs. Epochs for Hyperparameter Combinations')\n","plt.xlabel('Epochs')\n","plt.ylabel('Validation Loss')\n","plt.legend()\n","plt.show()\n","\n"],"metadata":{"id":"JK7mILRwtOTF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["best_hyperparams = None\n","lowest_val_loss = float('inf')\n","\n","for params, results in hyperparam_results.items():\n","    min_val_loss = min(results['val_losses'])\n","    if min_val_loss < lowest_val_loss:\n","        lowest_val_loss = min_val_loss\n","        best_hyperparams = params\n","\n","best_dropout, best_weight_decay, best_patience, best_hidden_size = best_hyperparams\n","print(f\"Best Hyperparameters: Dropout={best_dropout}, Weight Decay={best_weight_decay}, \"\n","      f\"Patience={best_patience}, Hidden Size={best_hidden_size}\")\n","print(f\"Lowest Validation Loss: {lowest_val_loss:.4f}\")\n"],"metadata":{"id":"HOvbKZYHF4Ne"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## The best model found through paramter searching is, with dropout=0.5, weight decay=0, patience=5, and hidden size=256."],"metadata":{"id":"N2mpTeTTMD2-"}},{"cell_type":"code","source":["print(\"\\nRe-training the model with the best hyperparameters...\")\n","\n","train_loader = DataLoader(work_train_dataset, batch_size=64, shuffle=True)\n","val_loader = DataLoader(work_val_dataset, batch_size=64, shuffle=False)\n","\n","class GradePredictor(nn.Module):\n","    def __init__(self, feature_sizes, output_dim, dropout_rate, hidden_size):\n","        super().__init__()\n","        super(GradePredictor, self).__init__()\n","        self.feature_layers = nn.ModuleDict({\n","            col: nn.Sequential(\n","                nn.Linear(size, hidden_size),\n","                nn.ReLU(),\n","                nn.Dropout(dropout_rate)\n","            )\n","            for col, size in feature_sizes.items()\n","        })\n","        self.fc = nn.Sequential(\n","            nn.Linear(len(feature_sizes) * hidden_size, 128),\n","            nn.ReLU(),\n","            nn.Dropout(dropout_rate),\n","            nn.Linear(128, output_dim)\n","        )\n","\n","    def forward(self, features):\n","        # Process each feature independently\n","        feature_outputs = [layer(features[col]) for col, layer in self.feature_layers.items()]\n","\n","        # Concatenate all feature outputs\n","        combined = torch.cat(feature_outputs, dim=1)\n","        logits = self.fc(combined)\n","\n","        # Apply softmax and scale the results\n","        softmax_outputs = F.softmax(logits, dim=1)\n","        return softmax_outputs\n","\n","best_model = GradePredictor(\n","    feature_sizes=feature_sizes,\n","    output_dim=output_dim,\n","    dropout_rate=best_dropout,\n","    hidden_size=best_hidden_size\n",").to(device)\n","\n","optimizer = torch.optim.Adam(best_model.parameters(), lr=0.00001, weight_decay=best_weight_decay)\n","criterion = nn.MSELoss()\n","scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=best_patience, factor=0.5)\n","\n","train_losses, val_losses = [], []\n","best_val_loss = float('inf')\n","\n","for epoch in range(10):\n","    best_model.train()\n","    train_loss = 0.0\n","    for features, targets in train_loader:\n","        features = {col: features[col].to(device) for col in features}\n","        targets = targets.to(device)\n","        optimizer.zero_grad()\n","        outputs = best_model(features)\n","        loss = criterion(outputs, targets)\n","        loss.backward()\n","        optimizer.step()\n","        train_loss += loss.item()\n","    train_losses.append(train_loss / len(train_loader))\n","\n","    best_model.eval()\n","    val_loss = 0.0\n","    with torch.no_grad():\n","        for features, targets in val_loader:\n","            features = {col: features[col].to(device) for col in features}\n","            targets = targets.to(device)\n","            outputs = best_model(features)\n","            loss = criterion(outputs, targets)\n","            val_loss += loss.item()\n","    val_losses.append(val_loss / len(val_loader))\n","\n","    scheduler.step(val_losses[-1])\n","\n","    print(f\"Epoch {epoch+1}, Train Loss: {train_losses[-1]:.4f}, Val Loss: {val_losses[-1]:.4f}\")\n","\n","print(f\"\\nFinal Validation Loss with Best Hyperparameters: {val_losses[-1]:.4f}\")\n"],"metadata":{"id":"SldC1Im_GK0R"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import mean_absolute_error, mean_squared_error\n","\n","test_loader = DataLoader(work_test_dataset, batch_size=64, shuffle=False)\n","\n","best_model.eval()\n","test_loss = 0.0\n","all_predictions = []\n","all_targets = []\n","\n","with torch.no_grad():\n","    for features, targets in test_loader:\n","        features = {col: features[col].to(device) for col in features}\n","        targets = targets.to(device)\n","        outputs = best_model(features)\n","        loss = criterion(outputs, targets)\n","        test_loss += loss.item()\n","        all_predictions.append(outputs.cpu().numpy())\n","        all_targets.append(targets.cpu().numpy())\n","\n","test_loss = test_loss / len(test_loader)\n","print(f\"\\nFinal Test Loss: {test_loss:.4f}\")\n","\n","all_predictions = np.vstack(all_predictions)\n","all_targets = np.vstack(all_targets)\n","\n","mae = mean_absolute_error(all_targets, all_predictions)\n","rmse = mean_squared_error(all_targets, all_predictions, squared=False)\n","\n","print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n","print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")\n","\n","category_mae = []\n","category_rmse = []\n","\n","for i, grade in enumerate(grade_columns):\n","    mae_category = mean_absolute_error(all_targets[:, i], all_predictions[:, i])\n","    rmse_category = mean_squared_error(all_targets[:, i], all_predictions[:, i], squared=False)\n","    category_mae.append(mae_category)\n","    category_rmse.append(rmse_category)\n","    print(f\"{grade} - MAE: {mae_category:.4f}, RMSE: {rmse_category:.4f}\")\n"],"metadata":{"id":"rRVv5a6dGaxX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Conclusion\n","\n","With the batch size effect, it saw that faster convergence happen with the less number for the batch size, as it seems that the dataset is easy to predict.\n","\n","Also, through the investigation with different optimizer, we saw the effectiveness of Adam optimizer, which seems superior to other optimizers, including SGD and RMSprop in finding the optimal parameters.\n","\n","Lastly, with the extensive hyperparameter searching, we found the best model with dropout=0.5, weight decay=0, patience=5, and hidden size=256. Its best performance resulted in 0.0523 in MAE and 0.0685 in RMSE, which is significant improvement, compared to the previous baseline result although MAE is slightly worse than the baseline (0.0518, 0.0910 for MAE and RMSE, respectively). RMSE shows huge improvement in preventing the model itself from having outlier prediction."],"metadata":{"id":"JGFQFeYxMSFT"}},{"cell_type":"code","source":[],"metadata":{"id":"Q50yz0bWNbnT"},"execution_count":null,"outputs":[]}]}